# -*- coding: utf-8 -*-
"""ResDilGatCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m2JeK_y5T0rWjIONla_8V5K9O67pxV4U
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
from torch.autograd import Variable
import math
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import scipy
import os
from IPython.display import Image, display, clear_output
import time

from tensorflow.python.client import device_lib
device_lib.list_local_devices()

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive', force_remount=False)
# %cd /gdrive/My\ Drive/Colab\ Notebooks/data

# Load data
# recording_mels = np.loadtxt('recording_mels.csv', delimiter=',',dtype=float)
# label_mels = np.loadtxt('./MIC05/label_mels.csv', delimiter=',',dtype=np.int16)

# Load all the data in a dict to keep track of the patients
# data = {}
# studies = os.listdir()

# for study in studies:
#   print('Study: ',study)
#   try: 
#     label_mels = np.load(study+'/label_mels.npy')
#     rec_mels = np.load(study + '/recording_mels.npy')
#     # Populate dict
#     data[study] = {}
#     data[study]['label_mels'] = label_mels
#     data[study]['rec_mels'] = rec_mels 
#     data[study]['mel_timevec'] = np.arange(len(label_mels))*0.01+0.0125
#   except:
#     print(study, 'failed to load')

# del label_mels, rec_mels

'''
Frame builder function
'''
def buildFrames(label_mels,fl,fs):
  # Build the index for the 270 length samples 
  # L: length of entire frame
  # fl: frame length
  # fs: frame shift
  # fn: frame number, i.e. number of frames possible in L
  # foh: frame overhang in the end of frame L (in fraction)
  # returns all the non-speech frames followed by all the speech frames
  start_idx = np.where(label_mels[:-1] - label_mels[1:] == -1)[0]+1
  end_idx = np.where(label_mels[:-1] - label_mels[1:] == 1)[0]+1

  speech_idx = []
  non_speech_idx = []

  # For speech
  for s, e in zip(start_idx,end_idx):
      L = e-s
      if L <= fl:
          fn = 1
      else:
          fn = (L-fl)/fs+1
          # Check if the last frame will leave no overhang
          if fn-np.floor(fn) == 0:
              foh = (fl-fs)/fl
          else:
              foh = (fl-fs*( fn-np.floor(fn)) ) / (fl)
          # The frame must have at least 50% overhang with the end of L
          if foh >= 0.5:
              fn += 1
              
      # build indexes 
      for i in np.arange(fn):
          speech_idx.append( np.arange( s+(i*fs),s+fl+(i*fs), dtype=np.int ))
          
  # For non-speech
  for k in range(len(start_idx)+1):
      
      if k == 0:
        s = 0
        e = start_idx[k]
      elif k == len(start_idx):
        s = end_idx[k-1]
        e = len(label_mels)
      else: 
          s = end_idx[k-1]
          e = start_idx[k]

      L = e-s        
      if L <= fl:
          continue # we don't want speech in the non-speech frames
      else:
          fn = np.floor((L-fl)/fs+1)
              
      # build indexes 
      for i in np.arange(fn):
          non_speech_idx.append( np.arange( s+(i*fs),s+fl+(i*fs), dtype=np.int ))

  #Build frame labels
  label_frames = np.concatenate( (np.zeros(len(non_speech_idx),dtype=int),np.ones(len(speech_idx),dtype=int)) ) 

  return non_speech_idx + speech_idx, label_frames


  '''
  Downsampler function
  '''
def randomDownsampler(label_frames):
  non_speech_idx = np.where(label_frames == 0)[0]
  speech_idx = np.where(label_frames == 1)[0]
  # Randomly sample the larger class without replacement
  non_speech_subsample = np.random.choice(non_speech_idx,len(speech_idx),replace=False)
  samples = np.concatenate((non_speech_subsample,speech_idx))
  return SubsetRandomSampler(samples)

'''
Segment data in frames and wrap them in Dataset and Dataloader
'''

# Make time vector in seconds for each mel frame
#mel_timevec = np.arange(len(label_mels))*0.01+0.0125
#labels = np.concatenate( (np.zeros(len(speech_mel_frames),dtype=int), np.ones(len(non_speech_mel_frames), dtype=int)) )

class MelDataset(Dataset):
    def __init__(self,study_names):
        # Initialize 
        self.rec_mels = np.empty((0,40))
        self.rec_frames = []
        self.label_frames = np.empty(0)
        self.contained_studies = []
        self.start_end_frame_idx = {}
        self.rec_len = {}

        # Create frames and concatenate
        prev_study_end = 0
        for study in study_names:
          # load studies and make frames
          print('Loading study '+ study + '..')
          label_mels = np.load(study+'/label_mels.npy')
          rec_mels = np.load(study + '/recording_mels.npy')
          rec_mels = rec_mels - np.mean(rec_mels,axis=0)
          rec_frames, label_frames = buildFrames(label_mels, fl=270, fs=90)
          # Concate frames
          self.rec_mels = torch.from_numpy( np.concatenate( (self.rec_mels, rec_mels),axis = 0 ) ).float()
          self.rec_frames = self.rec_frames + rec_frames
          self.label_frames = torch.from_numpy(np.concatenate( (self.label_frames, label_frames),axis = 0 )).type(torch.LongTensor)
          self.contained_studies.append(study)
          # make a dict for each study start and end index
          self.start_end_frame_idx[study] = {}
          self.start_end_frame_idx[study]['start'] = prev_study_end
          self.start_end_frame_idx[study]['end'] = prev_study_end + len(rec_frames)
          prev_study_end = self.start_end_frame_idx[study]['end']
          # save the length of the recording to contruct a time vector
          self.rec_len[study] = len(label_mels)
          
    def __len__(self):
        return len(self.label_frames)
    
    def __getitem__(self,idx):
      idx_range = self.rec_frames[idx]
      return np.expand_dims(self.rec_mels[idx_range,:],axis=0), self.label_frames[idx]


studies = []
# find all non-empty folders
for study in os.listdir():
  if not len(os.listdir('./'+study)) == 0:
    studies.append(study)

train_data = MelDataset(studies[:-3])
trainLoader = DataLoader(train_data, batch_size=16,sampler=randomDownsampler(train_data.label_frames))

# Dedicate at test set
test_data = MelDataset(studies[-3:])
testLoader = DataLoader(test_data, batch_size=16,sampler=randomDownsampler(test_data.label_frames))
# for i, (X,y) in enumerate(trainLoader):
#   print(i, X.shape, y, y.dtype) 
#   if i == 1:
#     break

'''
Main build block of network: Residual, dilated, gated CNN 
'''

class RCNNBlock(nn.Module):
  def __init__(self,input_channels,dilation,kernel_size,
               padding=None, bias=True, dropout=0.05):
    super(RCNNBlock,self).__init__()

    # weight inits
    def init_weights(m):
          if type(m) == nn.Conv2d:
              torch.nn.init.xavier_normal_(m.weight)
              m.bias.data.fill_(0.01)
    self.dropout = dropout

    if padding is None:
      padding = (kernel_size - 1)//2 * dilation
    
    # Convolutional layer
    self.convLayer = nn.Conv2d(in_channels=input_channels,
                  out_channels=input_channels*2, 
                  dilation=dilation, kernel_size=kernel_size, 
                  padding=padding, bias=bias)
    # For batchnorm
    self.BN2d_a = nn.BatchNorm2d(input_channels,eps=1e-05, momentum=0.1, affine=True)
    self.BN2d_b = nn.BatchNorm2d(input_channels,eps=1e-05, momentum=0.1, affine=True)
    
    # Initialize weights
    self.convLayer.apply(init_weights)

  
  def forward(self,x):
    res = x
    x = F.dropout(x, p=self.dropout, training=self.training)
    x = self.convLayer(x)
    a, b = torch.chunk(x, 2, dim=1)
    a = self.BN2d_a(a)
    b = self.BN2d_b(b)
    x = torch.tanh(a) * torch.sigmoid(b)
    return x + res

'''
Network architecture and test with dummy data
'''

# Check if CUDA is available 
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# Assuming that we are on a CUDA machine, this should print a CUDA device:
print(device)

image_channels = 1
num_classes = 2
gate_channels = 64
num_layers = 8
num_stacks = 2
kernel_size = 3
dropout=0.2
batch_size = 32

weight_decay = 1e-5
learning_rate = 1e-5

class Net(nn.Module):
    def __init__(self, cin_channels, gate_channels, num_classes, num_layers, 
                 num_stacks, kernel_size, dropout):
        super(Net, self).__init__()
        
        self.num_classes = num_classes
        self.cin_channels = cin_channels
        self.gate_channels = gate_channels
        self.kernel_size = kernel_size
        self.dropout = dropout
        self.num_stacks = num_stacks
        self.num_layers = num_layers

        # input block
        self.convBlock0 = nn.Conv2d(in_channels=cin_channels,
                                out_channels=gate_channels, 
                                kernel_size=kernel_size, 
                                padding=(kernel_size-1)//2)
        
        # Main convolutional layers
        assert num_layers % num_stacks == 0
        layers_per_stack = num_layers // num_stacks

        self.convLayers = nn.ModuleList()
        for layer in range(num_layers):
          dilation = 2**(layer % layers_per_stack)
          conv =  RCNNBlock(input_channels=gate_channels, 
                            dilation=dilation,kernel_size=kernel_size,dropout=dropout)
          self.convLayers.append(conv)

        # Dim reduction before DNN
        self.conv1D = nn.Conv2d(in_channels=gate_channels,
                                out_channels=2,
                                kernel_size = 1,
                                padding=0)
        self.BN0 = nn.BatchNorm2d(cin_channels,eps=1e-05, momentum=0.1, affine=True)
        
        # Fully connected
        self.fc1 = nn.Linear(21600, 8192)
        self.fc2 = nn.Linear(8192,4096)
        self.fc3 = nn.Linear(4096,num_classes)
        
        self.bn0 = nn.BatchNorm1d(21600)
        self.bn1 = nn.BatchNorm1d(8192)
        self.bn2 = nn.BatchNorm1d(4096)

        self.fc_do = nn.Dropout(p=dropout)
        
        nn.init.xavier_normal_(self.fc1.weight)
        nn.init.xavier_normal_(self.fc2.weight)
        nn.init.xavier_normal_(self.fc3.weight)

       

    def forward(self, x):
        #x = self.BN0(x)
        #x = self.convBlock0(x)
        x = x.repeat(1,self.gate_channels,1,1)
        #x = F.relu(x)

        x_res = x
        for layer in self.convLayers:
          x = layer(x)
          x_res = x_res + x
        x_res = x_res * math.sqrt(1/len(self.convLayers))
        x = self.conv1D(x)

        # Fully connected
        x = x.view(-1,self.num_flat_features(x))
        x = self.bn0(x)
        x = self.fc_do(x)
        
        x = self.fc1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.fc_do(x)
        
        x = self.fc2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.fc_do(x)
        
        x = self.fc3(x)
        
        return F.softmax(x, dim = 0) 
    
    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1 # initialize
        for s in size:
            num_features *= s
        return num_features
    
    def print_net_config(self):
        print('-- Network Configuration: --')
        print('gate_channels =',gate_channels) 
        print('kernel_size =', kernel_size) 
        print('dropout =', dropout)
        print('num_stacks =', num_stacks)
        print('num_layers =', num_layers)
        print('batch size =', batch_size)
        print('-- Optimizer: --')
        print('Optimizer: Adam')
        print('weight_decay =',weight_decay)
        print('learning rate =',learning_rate)

    

net = Net(cin_channels=image_channels, gate_channels=gate_channels, num_classes=num_classes,
          num_layers=num_layers,num_stacks=num_stacks, kernel_size=kernel_size, dropout=dropout)
net.to(device)
print(net)



criterion = nn.CrossEntropyLoss()  
optimizer = optim.Adam(net.parameters(), lr = learning_rate,  weight_decay=weight_decay)  

test_in = torch.rand(5,1,270,40)
net.eval()
test_out = net(Variable(test_in.to(device)))

print(test_out.shape)
print(test_out)

_, predicted = torch.max(test_out.data, 1)
print(predicted)

'''
Function for saving a network checkpoints
'''
def saveNetCheckpoint(net, suffix):
  # save the necessary input for the constructor
  config_dict = {'num_classes': net.num_classes,
                'cin_channels': net.cin_channels,
                'gate_channels': net.gate_channels,
                'kernel_size': net.kernel_size,
                'dropout': net.dropout,
                'num_stacks': net.num_stacks,
                'num_layers': net.num_layers}
  # Save the network weights and current results
  torch.save({
            'epoch_vec': epoch_vec,
            'model_state_dict': net.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'config_dict': config_dict,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accs': train_accs,
            'val_accs': val_accs,
            }, '../network_checkpoints/checkpoint_' + suffix + '.pt')

# studies = []
# # find all non-empty folders
# for study in os.listdir():
#   if not len(os.listdir('./'+study)) == 0:
#     studies.append(study)

# train_data = MelDataset(studies[:-3])
# trainLoader = DataLoader(train_data, batch_size=batch_size,sampler=randomDownsampler(train_data.label_frames))

# # Dedicate at test set
# test_data = MelDataset(studies[-3:])
# testLoader = DataLoader(test_data, batch_size=batch_size,sampler=randomDownsampler(test_data.label_frames))

# print(train_data.contained_studies)
# print(test_data.contained_studies)
# for key in checkpoint.keys():
#   print(key)

# checkpoint['epoch_vec']

'''
Load checkpoint or make new run 
'''
checkpoint_bool = False
if checkpoint_bool:
  checkpoint = torch.load('../network_checkpoints/checkpoint_best_val.pt')
  net = Net(**checkpoint['config_dict'])
  net.to(device)
  criterion = nn.CrossEntropyLoss()  
  optimizer = optim.Adam(net.parameters(), lr = learning_rate,  weight_decay=weight_decay)
  # Load states
  net.load_state_dict(checkpoint['model_state_dict'])
  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
  # Load epochs
  epoch_vec = checkpoint['epoch_vec']
  epoch_start = epoch_vec[-1]+1
  num_epoch = 20 + epoch_start
  # Load results
  train_losses = checkpoint['train_losses']
  val_losses = checkpoint['val_losses']
  train_accs = checkpoint['train_accs']
  val_accs = checkpoint['val_accs']
  # Setup the val acc to beat
  best_acc = val_losses[-1]
else:    
  train_losses = []
  val_losses = []
  train_accs = []
  val_accs = []

  num_epoch = 40
  epoch_start = 0
  best_acc = 0.0

'''
Network traning and testing 
'''
tmp_img = "tmp_vae_out.png"

first_run = True # To show the initial results of the network

'''
Train network loop
'''
for epoch in range(epoch_start, num_epoch):  

    running_loss = []
    running_acc = []
    running_precision = [] 
    running_recall = [] 
    running_fscore = [] 
    running_support = []

    t0 = time.time()

    # Show initial disitribution
    if first_run:
      net.eval()
      for inputs, labels in testLoader:
          output = net(Variable(inputs.cuda()))
          _, predicted = torch.max(output.data, 1)
          loss = criterion(output.cpu(),labels.cpu())
          running_loss.append(loss.item())
          acc = accuracy_score(labels.cpu(), predicted.cpu())
          running_acc.append(acc)
          # precision, recall, fscore, support
          precision, recall, fscore, support = precision_recall_fscore_support(labels.cpu(), predicted.cpu())
          running_precision.append(precision)
          running_recall.append(recall)
          # running_fscore.append(fscore)
          # running_support.append(support)
      first_run = False

      print('Intial test acc: %.3f, precicsion: %.3f, recall: %.3f,' % (np.mean(running_acc),np.mean(running_precision),np.mean(running_recall)))
      running_loss = []
      running_acc = []
      running_precision = [] 
      running_recall = [] 
      running_fscore = [] 
      running_support = []
      
    # Training
    net.train()
    for i, (inputs, labels) in enumerate(trainLoader, 0):

        # wrap them in Variable
        inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        output = net(inputs)
        loss = criterion(output,labels)
        loss.backward()
        optimizer.step()
        
        # Acc
        _, predicted = torch.max(output.data, 1)
        acc = accuracy_score(labels.cpu(), predicted.cpu())
        running_acc.append(acc)
        running_loss.append(loss.item())

        # precision, recall, fscore, support
        precision, recall, fscore, support = precision_recall_fscore_support(labels.cpu(), predicted.cpu())
        running_precision.append(precision)
        running_recall.append(recall)
        # running_fscore.append(fscore)
        # running_support.append(support)

    # save network at checkpoints         
    if epoch % 10 == 0 and epoch != 0:
      saveNetCheckpoint(net, 'epoch_' + str(epoch))
    
    # Save training results
    train_losses.append(np.mean(running_loss))
    train_accs.append(np.mean(running_acc))

    print('Train acc: %.3f, precicsion: %.3f, recall: %.3f,' % (np.mean(running_acc),np.mean(running_precision),np.mean(running_recall)))
    running_loss = []
    running_acc = []
    running_precision = [] 
    running_recall = [] 
    running_fscore = [] 
    running_support = []
    
    # test current performance
    net.eval()
    for inputs, labels in testLoader:
        output = net(Variable(inputs.cuda()))
        _, predicted = torch.max(output.data, 1)
        loss = criterion(output.cpu(),labels.cpu())
        running_loss.append(loss.item())
        acc = accuracy_score(labels.cpu(), predicted.cpu())
        running_acc.append(acc)

        # precision, recall, fscore, support
        precision, recall, fscore, support = precision_recall_fscore_support(labels.cpu(), predicted.cpu())
        running_precision.append(precision)
        running_recall.append(recall)
        # running_fscore.append(fscore)
        # running_support.append(support)
    
    t1 = time.time()
    print('Test acc: %.3f, precicsion: %.3f, recall: %.3f,' % (np.mean(running_acc),np.mean(running_precision),np.mean(running_recall)))
   
    # Save test results
    val_losses.append(np.mean(running_loss))
    val_accs.append(np.mean(running_acc))
    # save best network 
    if best_acc < val_accs[-1] and epoch >= 5:
      best_acc = val_accs[-1]
      saveNetCheckpoint(net, 'best_val')
      # Write a log file
      text_file = open("../network_checkpoints/best_network.txt", "w")
      text_file.write('Best epoch: ' + str(epoch) + '\nacc = ' + str(best_acc))
      text_file.close()

    # save the model for the last epoch
    if epoch == num_epoch:
      saveNetCheckpoint(net, 'epoch_' + str(epoch))
    print('Epoch time: %.1f seconds' % (t1-t0))
    print('Estimated time to finnish: %.0f min %.0f sec' % ((t1-t0)*num_epoch//60, ((t1-t0)*num_epoch)%60))

    # print net config
    net.print_net_config()

    fig = plt.figure()
    ax1 = fig.add_subplot(1,2,1)
    epoch_vec = np.arange(len(train_losses))
    ax1.clear()
    ax1.plot(epoch_vec, train_losses, 'r', label='Train Loss')
    ax1.plot(epoch_vec, val_losses, 'b', label='Val Loss')
    ax1.legend()
    ax1.set_xlabel('Updates')
    ax1.set_ylabel('Loss')
    ax1.grid()

    ax2 = fig.add_subplot(1,2,2)
    ax2.plot(epoch_vec, train_accs, 'r', label='Train Acc')
    ax2.plot(epoch_vec, val_accs, 'b', label='Val Acc')
    ax2.legend()
    ax2.set_xlabel('Updates')
    ax2.set_ylabel('Accuracy')
    ax2.grid()
    
    plt.savefig(tmp_img)
    plt.close(fig)
    display(Image(filename=tmp_img))
    clear_output(wait=True)

    os.remove(tmp_img)



text_file = open("../network_checkpoints/best_network.txt", "w")
text_file.write('Best epoch: ' + str(epoch) + '\n acc = 0.78')
text_file.close()

'''
Function for saving a network checkpoint
'''
def saveNetCheckpoint(net, suffix):
  # save the necessary input for the constructor
  config_dict = {'num_classes': net.num_classes,
                'cin_channels': net.cin_channels,
                'gate_channels': net.gate_channels,
                'kernel_size': net.kernel_size,
                'dropout': net.dropout,
                'num_stacks': net.num_stacks,
                'num_layers': net.num_layers}
  # Save the network weights and current results
  torch.save({
            'epoch_vec': epoch_vec,
            'model_state_dict': net.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'config_dict': config_dict,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accs': train_accs,
            'val_accs': val_accs,
            }, '../network_checkpoints/checkpoint_' + suffix + '.pt')

from google.colab import drive
drive.mount('/content/drive')

fig = plt.figure()
ax1 = fig.add_subplot(1,2,1)
epoch_vec = np.arange(len(train_losses))
ax1.clear()
ax1.plot(epoch_vec, train_losses, 'r', label='Train Loss')
ax1.plot(epoch_vec[:-1], val_losses, 'b', label='Val Loss')
ax1.legend()
ax1.set_xlabel('Updates')
ax1.set_ylabel('Loss')
ax1.grid()

ax2 = fig.add_subplot(1,2,2)
ax2.plot(epoch_vec, train_accs, 'r', label='Train Acc')
ax2.plot(epoch_vec[:-1], val_accs, 'b', label='Val Acc')
ax2.legend()
ax2.set_xlabel('Updates')
ax2.set_ylabel('Accuracy')
ax2.grid()



print(len(epoch_vec))
print(len(val_accs))

'''
Run test set, save results and save as a .wav file inspection
'''
def testLabeler(testData, net):
  linearLoader = DataLoader(test_data, batch_size=16, shuffle = False, drop_last = False)
  net.eval()
  prediction_vec = np.empty(0)
  label_vec = np.empty(0)
  # Get predictions
  for i, (inputs, labels) in enumerate(linearLoader):
    output = net(Variable(inputs.cuda()))
    _, predicted = torch.max(output.data, 1)
    prediction_vec = np.concatenate((prediction_vec, output.cpu().detach().numpy()[:,1]))
    label_vec = np.concatenate((label_vec, labels.cpu().detach().numpy()))
    if i % 1000 == 0:
      print('Batch',i)
  # split the labels according to study and save results
  pred_dict = {}
  for study in testData.start_end_frame_idx.keys():
    pred_dict[study] = {}
    start_ = testData.start_end_frame_idx[study]['start']
    end_ = testData.start_end_frame_idx[study]['end']
    pred_dict[study]['predictions'] = prediction_vec[start_:end_]
    pred_dict[study]['labels'] = label_vec[start_:end_]
    pred_dict[study]['frames'] = testData.rec_frames[start_:end_]
    pred_dict[study]['mel_timevec'] =  np.arange(testData.rec_len[study])*0.01+0.0125
    pred_dict[study]['prediction_time'] =  [np.mean(pred_dict[study]['mel_timevec'][frame - start_]) for frame in pred_dict[study]['frames']]
  
  return pred_dict
    
def pred2wav(MIC_dict,study, save_path):
    from scipy.io import wavfile
    # get sorted index
    sort_idx = np.argsort(MIC_dict['prediction_time'])
    # Sort after it
    sort_pred_time = np.array(MIC_dict['prediction_time'])[sort_idx]
    sort_pred = np.array(MIC_dict['predictions'])[sort_idx]
    sort_labels = np.array(MIC_dict['labels'])[sort_idx]
    # interpolate  
    last_sample_time = MIC_dict['mel_timevec'][-1]
    interp_pred_time = np.linspace(0,last_sample_time, np.round(last_sample_time)*10) # make a grid
    interp_pred = scipy.interpolate.griddata(sort_pred_time,sort_pred,interp_pred_time, method='nearest')
    interp_true_labels = scipy.interpolate.griddata(sort_pred_time,sort_labels, interp_pred_time, method='nearest')
    # write file
    wavfile.write(save_path+study+'_pred_labels.wav', 1, interp_pred)
    wavfile.write(save_path+study+'_true_labels.wav', 1, interp_true_labels)

predDict = testLabeler(test_data, net)
for study in predDict.keys():
  pred2wav(predDict[study],study,'../pred_wav/')

start_idx = np.where(test_data.label_frames[:-1] - test_data.label_frames[1:] == -1)[0]+1
end_idx = np.where(test_data.label_frames[:-1] - test_data.label_frames[1:] == 1)[0]+1

print(start_idx)
print(end_idx)

import pickle
print(test_data.contained_studies[0])

MIC14 = {}
MIC14['predictions'] = prediction_vec[:28214+1]
MIC14['labels'] = labels_vec[:28214+1]
MIC14['frames'] = test_data.rec_frames[:28214+1]
MIC14['mel_timevec'] = np.load('./MIC14/mel_timevec.npy')
MIC14['prediction_time'] = [np.mean(MIC14['mel_timevec'][frame]) for frame in MIC14['frames']]

for key, val in MIC14.items():
  print(key, np.shape(val))


def save_obj(obj, name ):
    with open('../obj/'+ name + '.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_obj(name ):
    with open('../obj/' + name + '.pkl', 'rb') as f:
        return pickle.load(f)

save_obj(MIC14,'MIC14')

